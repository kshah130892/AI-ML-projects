{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7_hx8SJJR4-"
   },
   "source": [
    "# Assignment 2 - Recurrent Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AiWTVDf7cZ2"
   },
   "source": [
    "## Programming (Full points: 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, our goal is to use PyTorch to implement Recurrent Neural Networks (RNN) for sentiment analysis task. Sentiment analysis is to classify sentences (input) into certain sentiments (output labels), which includes positive, negative and neutral.\n",
    "\n",
    "We will use a benckmark dataset, SST, for this assignment.\n",
    "* we download the SST dataset from torchtext package, and do some preprocessing to build vocabulary and split the dataset into training/validation/test sets. You don't need to modify the code in this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# load data splits\n",
    "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL)\n",
    "\n",
    "# build dictionary\n",
    "TEXT.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#print(train_data)\n",
    "\n",
    "# hyperparameters\n",
    "vocab_size = len(TEXT.vocab)\n",
    "label_size = len(LABEL.vocab)\n",
    "padding_idx = TEXT.vocab.stoi['<pad>']\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# build iterators\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data), \n",
    "    batch_size=32)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'label'])\n",
      "dict_values([['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '``', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.'], 'positive'])\n",
      "[('positive', 3610), ('negative', 3310), ('neutral', 1624)]\n"
     ]
    }
   ],
   "source": [
    "#Printing the sample values in training data\n",
    "print(train_data[0].__dict__.keys())\n",
    "print(train_data[0].__dict__.values())\n",
    "print(LABEL.vocab.freqs.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of records in training data: 8544\n",
      "No of records in test data: 2210\n",
      "No of records in validation data: 1101\n",
      "No of records in training iteration: 267\n",
      "No of records in test iteration: 70\n",
      "No of records in validation iteration: 35\n",
      "Vocabulary size : 16583\n",
      "Number of classes : 3\n"
     ]
    }
   ],
   "source": [
    "#Printing number of records in training/Validation/testing \n",
    "print(f\"No of records in training data: {len(train_data)}\")\n",
    "print(f\"No of records in test data: {len(test_data)}\")\n",
    "print(f\"No of records in validation data: {len(val_data)}\")\n",
    "\n",
    "print(f\"No of records in training iteration: {len(train_iter)}\")\n",
    "print(f\"No of records in test iteration: {len(test_iter)}\")\n",
    "print(f\"No of records in validation iteration: {len(val_iter)}\")\n",
    "\n",
    "print(f\"Vocabulary size : {vocab_size}\")\n",
    "print(f\"Number of classes : {label_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the device to use\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* define the training and evaluation function in the cell below.\n",
    "### (25 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "def train_model(model, iterator):\n",
    "     model.train()\n",
    "     total_loss=[]\n",
    "     total_accuracy=[]\n",
    "     for i in range(epoch):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy=0  \n",
    "        loss_int=0\n",
    "        accuracy_int=0\n",
    "         \n",
    "        for batch in iterator:            \n",
    "            text = batch.text\n",
    "            label=batch.label\n",
    "            optimizer.zero_grad()            \n",
    "            out = model(text)        #Forward Pass   \n",
    "            loss = criterion(out, label)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() #Loss Calculation\n",
    "\n",
    "             # Calculate accuracy\n",
    "            predictions = out.argmax(1)\n",
    "            correct = (predictions == label).sum().item()\n",
    "            accuracy = correct / len(label)\n",
    "            epoch_accuracy+=accuracy\n",
    "\n",
    "        loss_int=\"{:.4f}\".format(epoch_loss / len(iterator))\n",
    "        accuracy_int=\"{:.4f}\".format(epoch_accuracy / len(iterator))\n",
    "        total_loss.append(loss_int)\n",
    "        total_accuracy.append(accuracy_int)\n",
    "        print(f'Epoch {i+1}/{epoch}, Loss: {loss_int}, Accuracy:{accuracy_int}')\n",
    "\n",
    "     return total_loss,total_accuracy \n",
    "\n",
    "#Evaluate the model\n",
    "def evaluate_model(model, iterator):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            label = batch.label\n",
    "            \n",
    "            # Forward pass\n",
    "            out = model(text)\n",
    "            \n",
    "            # Calculate loss and accuracy\n",
    "            loss = criterion(out, label)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = out.argmax(1)\n",
    "            correct = (predictions == label).sum().item()\n",
    "            accuracy = correct / len(label)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_accuracy += accuracy\n",
    "            \n",
    "    final_loss=\"{:.4f}\".format(epoch_loss / len(iterator))\n",
    "    final_accuracy=\"{:.4f}\".format(epoch_accuracy / len(iterator))\n",
    "    return final_loss, final_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* build a RNN model for sentiment analysis in the cell below.\n",
    "We have provided several hyperparameters we needed for building the model, including vocabulary size (vocab_size), the word embedding dimension (embedding_dim), the hidden layer dimension (hidden_dim), the number of layers (num_layers) and the number of sentence labels (label_size). Please fill in the missing codes, and implement a RNN model.\n",
    "### (40 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.label_size = label_size\n",
    "        self.num_layers = 1\n",
    "        \n",
    "        # add the layers required for sentiment analysis.\n",
    "        # Embedding Layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx) \n",
    "        #RNN Layer\n",
    "        self.rnn=nn.RNN(embedding_dim,hidden_dim, num_layers=self.num_layers,batch_first=True)\n",
    "        #output Layer\n",
    "        self.output = nn.Linear(hidden_dim, label_size)\n",
    "        # Activation function for classification\n",
    "        self.activation = nn.Sigmoid() if label_size == 1 else nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def zero_state(self, batch_size): \n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)  #hidden state\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)  # cell state\n",
    "        return hidden,cell\n",
    "\n",
    "    def forward(self, text):\n",
    "        #implement the forward function of the model.\n",
    "        embedding = self.embedding(text)\n",
    "\n",
    "        #Initializing hidden state \n",
    "        batch_size=text.size(0)\n",
    "        #print(batch_size)\n",
    "        hidden,cell=self.zero_state(batch_size)\n",
    "\n",
    "        #RNN\n",
    "        out,hidden=self.rnn(embedding,hidden)\n",
    "\n",
    "        #output layer\n",
    "        hidden=hidden[-1]\n",
    "        out=self.activation(self.output(hidden))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train the model and compute the accuracy in the cell below.\n",
    "### (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with training Iteration\n",
      "Epoch 1/5, Loss: 1.0547, Accuracy:0.4222\n",
      "Epoch 2/5, Loss: 1.0473, Accuracy:0.4225\n",
      "Epoch 3/5, Loss: 1.0464, Accuracy:0.4243\n",
      "Epoch 4/5, Loss: 1.0455, Accuracy:0.4217\n",
      "Epoch 5/5, Loss: 1.0452, Accuracy:0.4239\n",
      "Training the model with Validation Iteration\n",
      "Epoch 1/5, Loss: 1.0146, Accuracy:0.5038\n",
      "Epoch 2/5, Loss: 1.0003, Accuracy:0.5400\n",
      "Epoch 3/5, Loss: 0.9706, Accuracy:0.5525\n",
      "Epoch 4/5, Loss: 0.9543, Accuracy:0.5609\n",
      "Epoch 5/5, Loss: 0.9372, Accuracy:0.5707\n",
      "Evaluating the model with testing Iteration\n",
      "Test Loss: 1.0615, Test Accuracy: 52.32%\n"
     ]
    }
   ],
   "source": [
    "#Initializing paramters\n",
    "learning_rate=0.0001\n",
    "epoch=5\n",
    "batch_size=32\n",
    "output_dim=3\n",
    "\n",
    "#Model Defination\n",
    "model=RNNClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx).to(device)\n",
    "model=model.to(device)\n",
    "\n",
    "#Optimizer and Loss function\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "     \n",
    "#Training Model with Training Iteration\n",
    "print(\"Training the model with training Iteration\")\n",
    "training_loss, training_accuracy=train_model(model,train_iter)\n",
    "\n",
    "#Training Model with Validation Iteration\n",
    "print(\"Training the model with Validation Iteration\")\n",
    "valid_loss,valid_accuracy=train_model(model,val_iter)\n",
    "\n",
    "#Evaluating the model with testing Iteration\n",
    "print(\"Evaluating the model with testing Iteration\")\n",
    "test_loss, test_accuracy = evaluate_model(model, test_iter)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {float(test_accuracy)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* try to train a model with better accuracy in the cell below. For example, you can use different optimizers such as SGD and Adam. You can also compare different hyperparameters and model size.\n",
    "### (15 points), to obtain FULL point in this problem, the accuracy needs to be higher than 70%\n",
    "**Answer:**<br>\n",
    "There is a new file containing the code which tries to improve the accuracy of the model than the current one.\n",
    "Created new file for better understanding.<br>\n",
    "File Name - Assign2_LSTM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
